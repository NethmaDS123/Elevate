// ------------------
// Detailed Explanation Data
// ------------------

const coreTopicExplanations: { [key: string]: string } = {
  Arrays:
    "Arrays are contiguous memory structures used to store multiple elements of the same type. They allow fast random access (O(1)) by index. They're fundamental for implementing more complex data structures like hash tables, stacks, queues, and heaps. Common operations include insertion, deletion, searching, and sorting.",
  "Linked Lists":
    "Linked lists are linear data structures composed of nodes, where each node contains data and a reference (pointer) to the next node. They provide efficient insertion and deletion (O(1)) at any point in the list, but searching remains linear (O(n)). They're useful for implementing dynamic data structures like stacks, queues, and graphs.",
  Trees:
    "Trees are hierarchical structures consisting of nodes connected by edges, starting from a root node. Common types include binary trees, binary search trees (BST), AVL trees, and heaps. Trees enable efficient data operations like insertion, deletion, and lookup, typically in O(log n), and are crucial for representing hierarchical data such as file systems and databases.",
  Graphs:
    "Graphs are flexible data structures composed of nodes (vertices) and connections (edges). They model complex relationships and networks, such as social media interactions, GPS navigation, and web page linking. Algorithms used with graphs include breadth-first search (BFS), depth-first search (DFS), Dijkstra's algorithm, and topological sorting.",
  "Hash Tables":
    "Hash tables are data structures providing extremely fast lookup, insertion, and deletion (O(1) average case). They work by computing a hash function on keys, which maps the data to specific indices in an array. Hash tables underpin many data-intensive applications, like databases, caches, and language interpreters.",
  Stacks:
    "Stacks follow a Last-In-First-Out (LIFO) principle, meaning the last item added is the first one to be removed. Typical operations are push, pop, and peek, all performed in O(1). Stacks are critical in scenarios involving recursion, backtracking algorithms, expression evaluation, syntax parsing, and undo operations.",
  Queues:
    "Queues follow a First-In-First-Out (FIFO) structure, meaning the earliest inserted item is the first to be removed. Operations like enqueue, dequeue, and peek run efficiently in O(1). Queues are essential for task scheduling, buffering, streaming data, and breadth-first traversal of trees and graphs.",
  Heaps:
    "Heaps are specialized binary trees used primarily for priority queue implementations. They efficiently provide quick access (O(1)) to the maximum or minimum element, while insertion and deletion operations take O(log n). Heaps are often utilized in algorithms like heap sort, Dijkstra’s shortest-path, and priority-driven scheduling.",
  Recursion:
    "Recursion is a programming technique where a function calls itself to break a problem into smaller subproblems. It simplifies complex problems, particularly those naturally hierarchical or repetitive, such as traversing trees/graphs, divide-and-conquer algorithms, dynamic programming, permutations, and backtracking solutions.",
  Greedy:
    "Greedy algorithms iteratively make locally optimal choices, hoping to achieve a globally optimal solution. They are efficient and simple to implement but don't guarantee optimality for all problems. Greedy methods are particularly useful for problems like coin change, job scheduling, Huffman encoding, and finding minimum spanning trees.",
  "Dynamic Programming":
    "Dynamic programming solves complex problems by breaking them into simpler overlapping subproblems. It stores solutions to these subproblems (memoization) to avoid redundant computations, thus improving efficiency drastically (often from exponential to polynomial complexity). Common examples include Fibonacci sequences, knapsack problems, longest common subsequences, and matrix-chain multiplication.",
  "Sorting Algorithms":
    "Sorting algorithms arrange data into an ordered sequence. Common algorithms include Quick Sort, Merge Sort, Heap Sort, and Bubble Sort. Efficient sorting is crucial as it optimizes subsequent search operations and data organization. They vary widely in complexity, ranging from O(n log n) for efficient algorithms (Quick Sort, Merge Sort) to O(n²) for simpler algorithms like Bubble Sort.",
  "Searching Algorithms":
    "Searching algorithms find specific elements within data structures. Common examples include Linear Search (O(n)), Binary Search (O(log n)), and specialized algorithms like BFS and DFS for trees and graphs. Efficient searching is fundamental for data retrieval tasks, indexing systems, and database operations.",
  "Bit Manipulation":
    "Bit manipulation involves performing operations directly on binary digits (bits) of integers. It includes operations like AND, OR, XOR, shifting, and masking. Bit manipulation is highly efficient, commonly utilized in embedded systems, low-level programming, cryptography, error detection algorithms, and performance-critical applications.",
};

const leetCodePatternExplanations: { [key: string]: string } = {
  "Sliding Window":
    "The Sliding Window technique solves problems involving contiguous subarrays or substrings by maintaining two pointers or indices that represent the bounds of a 'window'. The window dynamically expands or shrinks based on the problem's conditions, optimizing time complexity from O(n²) to O(n). Common applications include substring search, maximum subarray sums, and finding the longest unique character substring.",
  "Two Pointers":
    "The Two Pointers technique leverages two pointers traversing a data structure from different directions or at varying speeds, effectively reducing unnecessary computations. It’s often employed in sorted arrays or linked lists to find pairs meeting certain conditions (e.g., sum pairs, palindrome checks, or detecting cycles), usually with linear (O(n)) complexity.",
  "Merge Intervals":
    "Merge Intervals solves problems involving overlapping intervals by sorting intervals based on their start or end points and merging overlaps sequentially. It's commonly applied in scheduling scenarios, meeting rooms allocation, and calendar event management. The typical complexity is O(n log n), driven by the initial sorting step.",
  "Tree BFS":
    "Tree Breadth-First Search (BFS) explores nodes level-by-level using a queue, making it ideal for finding shortest paths or minimum depth in trees and graphs. It guarantees the shortest path in unweighted graphs and is often used in network broadcasting, social networks traversal, and shortest-path algorithms. Complexity is typically O(V + E) for graphs.",
  "Tree DFS":
    "Tree Depth-First Search (DFS) explores nodes by going deep into each branch before backtracking. DFS can be implemented recursively or using a stack. It's extensively used in path-finding, topological sorting, detecting cycles, and exploring connected components. DFS's complexity is usually O(V + E) in graphs, making it efficient for traversal and exhaustive search problems.",
  "Binary Search":
    "Binary Search is an efficient divide-and-conquer algorithm for locating an element within a sorted array by repeatedly dividing the search space in half, reducing the complexity from O(n) (linear search) to O(log n). This technique is crucial for problems involving sorted data or searching within specific numeric ranges.",
  "Dynamic Programming":
    "Dynamic Programming (DP) solves complex problems by breaking them down into simpler overlapping subproblems, solving each subproblem only once (via memoization or tabulation) and storing their solutions to avoid redundant calculations. DP is especially useful for optimization problems, such as longest common subsequence, edit distance, and knapsack problems. DP solutions typically improve complexity from exponential to polynomial time.",
  Backtracking:
    "Backtracking systematically searches for all or some solutions to problems by incrementally building candidates and abandoning them ('backtracking') once they fail to satisfy the problem constraints. It's commonly used for constraint satisfaction problems, such as puzzles, permutations, combinations, Sudoku, and N-Queens problems. While the worst-case complexity can be exponential, pruning strategies often optimize performance.",
  Greedy:
    "Greedy algorithms solve optimization problems by making locally optimal choices at each step, with the hope that these choices lead to a globally optimal solution. They're often simpler and faster than dynamic programming but don't guarantee optimality for all problems. Common uses include activity selection, coin change (in specific currencies), Huffman coding, and minimum spanning trees.",
  "Fast and Slow Pointers":
    "The Fast and Slow Pointers pattern uses two pointers moving through a structure at different speeds (often one at twice the speed of the other). It's effective for cycle detection (Floyd’s Tortoise and Hare algorithm), finding middle elements, and determining palindromes in linked lists. The approach has linear complexity (O(n)) and minimal memory usage.",
  "Topological Sort":
    "Topological sorting orders nodes in a Directed Acyclic Graph (DAG) so that every directed edge from node A to node B implies A appears before B. This pattern is crucial for scheduling tasks, detecting cycles in dependencies, and build systems. Typical algorithms include Kahn's algorithm and DFS-based methods, both with O(V + E) complexity.",
  "Bit Manipulation":
    "Bit Manipulation involves direct operations on individual bits of binary numbers (e.g., AND, OR, XOR, bit shifting). It's efficient in memory and computational speed, crucial for solving problems related to bitwise operations, subset generation, missing numbers, single-number identification, and low-level systems programming tasks. Operations usually have O(1) time complexity.",
  "Union Find":
    "Union-Find (Disjoint Set Union) is used to manage disjoint sets, efficiently performing two operations: finding the set a particular element belongs to, and uniting two sets. Commonly used in graph connectivity, cycle detection, and clustering algorithms, it achieves near-constant complexity per operation (approximately O(α(n)), where α is the inverse Ackermann function).",
  "Prefix Sum":
    "Prefix Sum (or cumulative sum) stores the sum of elements up to each index, enabling rapid calculations of range sums or averages. It optimizes range query problems from O(n²) to O(n). Frequently applied in subarray sum problems, range sum queries, and certain optimization scenarios like the equilibrium index problem.",
};

const studyRoadmapExplanations: { [key: string]: string } = {
  "Step 1: Master Fundamental Data Structures":
    "Gain complete proficiency in essential data structures such as Arrays, Linked Lists, Stacks, Queues, and Hash Tables. Understand their operations, complexity, implementation details, and common applications. Solve problems to master operations like insertion, deletion, traversal, and searching.",
  "Step 2: Develop Algorithmic Foundations":
    "Deeply understand key algorithmic concepts like Sorting (Merge Sort, Quick Sort, Heap Sort), Searching (Binary Search, Linear Search), and Recursion. Learn how to analyze complexity (time and space) rigorously. Solve a wide range of basic algorithm problems to internalize these concepts thoroughly.",
  "Step 3: Advance into Trees and Graphs":
    "Study advanced data structures including Binary Trees, Binary Search Trees, Heaps, AVL Trees, and Graphs. Practice key algorithms such as Breadth-First Search (BFS), Depth-First Search (DFS), Dijkstra's algorithm, Topological Sorting, and Graph traversals. Focus on understanding problem-solving techniques involving these structures deeply.",
  "Step 4: Master Dynamic Programming and Recursion":
    "Intensively practice Dynamic Programming (DP) and Recursive algorithms. Identify overlapping subproblems, optimal substructure properties, memoization, and tabulation techniques. Solve classic DP problems (Knapsack, Longest Common Subsequence, Fibonacci sequences) and strengthen your recursive reasoning.",
  "Step 5: Solve Pattern-Based Problems":
    "Master commonly tested LeetCode problem patterns including Sliding Window, Two Pointers, Merge Intervals, Fast and Slow Pointers, and Prefix Sums. Recognize patterns quickly, enabling you to approach unseen problems confidently and effectively during interviews.",
  "Step 6: Enhance Problem-Solving Speed and Accuracy":
    "Regularly practice mock interviews under timed conditions. Aim to solve medium-level questions within 20-30 minutes, clearly explaining your approach. Review solutions thoroughly to understand optimal solutions, edge cases, and alternative approaches. Aim for consistent accuracy and efficient coding style.",
  "Step 7: Build System Design Foundations":
    "Gain foundational knowledge of System Design, including scalability, load balancing, caching, database management, API design, microservices architecture, and distributed systems. Understand how to approach open-ended design questions systematically, articulate trade-offs, and propose scalable solutions clearly.",
  "Step 8: Apply Knowledge to Real-World Projects":
    "Work on real-world software engineering projects or personal side-projects. Engage in full-stack or backend development, using technologies relevant to your targeted roles. Practical experience demonstrates proficiency and deepens understanding of technical concepts used during technical discussions.",
  "Step 9: Prepare for Behavioral and Communication Skills":
    "Refine your ability to clearly explain technical solutions and past project experiences using structured frameworks like STAR. Demonstrate critical soft skills such as communication, collaboration, leadership, and adaptability. Practice common behavioral questions frequently asked by top tech companies.",
  "Step 10: Engage in Continuous Improvement":
    "Constantly refine and revisit your weaker areas. Regularly participate in mock interviews, analyze your performance, and iterate. Review past interview experiences, gather feedback, and continuously adapt your preparation strategy to address identified gaps, optimizing your readiness for interviews at leading tech companies.",
};

const behavioralQuestionExplanations: { [key: string]: string } = {
  "Tell me about a challenge you overcame":
    "Explain a real-life challenge, how you approached it, the actions you took, and what you learned. Focus on problem-solving and resilience.",
  "Describe a leadership experience":
    "Detail a scenario where you led a team or project, the challenges you faced, and the impact your leadership had on the outcome.",
  "How do you handle conflicting priorities?":
    "Discuss your approach to time management, prioritization, and communication when dealing with multiple tasks or conflicts.",
  "Can you give an example of a time you failed?":
    "Share a failure and the steps you took to overcome it, what you learned, and how it has influenced your work ethic.",
};

const starMethodExplanation = `The STAR method is a structured manner of responding to behavioral interview questions by discussing the:
  - **Situation**: Set the context and describe the background.
  - **Task**: Explain the task or challenge that was involved.
  - **Action**: Describe the specific actions you took to address it.
  - **Result**: Share the outcomes or results of your actions, quantifying your success when possible.
  This approach helps in delivering clear and concise answers.`;
